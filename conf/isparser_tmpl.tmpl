from tools.LogGenerator import ML, make_logfile
from tools.TaskGenerator import upload_task


class Parser:
    """
    requests_html is encapsulated here. For details, you can move to:
        https://github.com/kennethreitz/requests-html;
        https://pypi.org/project/requests-html/

    @:param spider_tmplName: The className of the corresponding crawler template, from < set_spiders.py >
    @:param url: Target website
    @:param methodname: Select the parsing method you need to use
    """
    spider_tmplName = None
    methodname = "xpath"
    url = "you crawled url"

    def parser_source(self, p):
        """
        Show your cleaning data here, like this:
            info = p('//*[@id="articleContentId"]/text()')
        """
        pass


if __name__ == '__main__':
    #  You can initialize your log file
    make_logfile('is_test')

    ML.info('Start running...')
    upload_task(tmplName=Parser.spider_tmplName, taskUrl=Parser.url)
